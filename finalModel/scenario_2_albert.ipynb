{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/smaller-models-docs/blob/main/nlp/cook/fine-tune/albert_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZrl7EHuJUfG"
   },
   "source": [
    "# Text Classification with Transformers (ALBERT)\n",
    "\n",
    "This script helps you fine-tune a pre-trained model (ALBERT) and encoder model for text classification with a dataset from the HuggingFace.\n",
    "\n",
    "The use case uses binary classes to produce a model to identify clickbait versus factual content with the use of a synthetic dataset found [here](https://huggingface.co/datasets/ilsilfverskiold/clickbait_titles_synthetic_data). This script follows a tutorial that you can find here.\n",
    "\n",
    "You may use any encoder model such as BERT, RoBERTa and DeBERTa instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvzGqWjSW2sd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T02:35:01.469226Z",
     "iopub.status.busy": "2024-07-17T02:35:01.468851Z",
     "iopub.status.idle": "2024-07-17T02:35:03.929649Z",
     "shell.execute_reply": "2024-07-17T02:35:03.928613Z",
     "shell.execute_reply.started": "2024-07-17T02:35:01.469189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB6zvHZFJFMJ"
   },
   "source": [
    "Import the dataset you'll be trainin on. This dataset has a 'text' field and a 'label' field. Be sure to tweak the script if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:09:37.610887Z",
     "iopub.status.busy": "2024-07-17T03:09:37.610578Z",
     "iopub.status.idle": "2024-07-17T03:09:37.993956Z",
     "shell.execute_reply": "2024-07-17T03:09:37.993226Z",
     "shell.execute_reply.started": "2024-07-17T03:09:37.610863Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:09:39.153204Z",
     "iopub.status.busy": "2024-07-17T03:09:39.152793Z",
     "iopub.status.idle": "2024-07-17T03:09:39.240987Z",
     "shell.execute_reply": "2024-07-17T03:09:39.240301Z",
     "shell.execute_reply.started": "2024-07-17T03:09:39.153177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cpl.org.pe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faithandreason.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kimhauser.ch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the national association for honesty in medici...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lokoml.cz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                         cpl.org.pe      0\n",
       "1                                 faithandreason.com      0\n",
       "2                                       kimhauser.ch      0\n",
       "3  the national association for honesty in medici...      1\n",
       "4                                          lokoml.cz      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "set_1_df = pd.read_csv('set_1_df.csv')  # Adjust the file path as needed\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "set_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:09:46.429085Z",
     "iopub.status.busy": "2024-07-17T03:09:46.428771Z",
     "iopub.status.idle": "2024-07-17T03:09:46.456259Z",
     "shell.execute_reply": "2024-07-17T03:09:46.455569Z",
     "shell.execute_reply.started": "2024-07-17T03:09:46.429061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear valued applicant,\\n\\nWe are pleased to in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://pub-65f0ebbc2188423eae437d24914d92a5.r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear [Recipient],\\n\\nWe are pleased to announc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Ami,\\n\\nI am forwarding the October nomin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Exmh-workers mailing list,\\n\\nI wanted to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Dear valued applicant,\\n\\nWe are pleased to in...      1\n",
       "1  https://pub-65f0ebbc2188423eae437d24914d92a5.r...      1\n",
       "2  Dear [Recipient],\\n\\nWe are pleased to announc...      0\n",
       "3  Dear Ami,\\n\\nI am forwarding the October nomin...      0\n",
       "4  Dear Exmh-workers mailing list,\\n\\nI wanted to...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "set_3_df = pd.read_csv('set_3_df.csv')  # Adjust the file path as needed\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "set_3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:10:05.164607Z",
     "iopub.status.busy": "2024-07-17T03:10:05.164288Z",
     "iopub.status.idle": "2024-07-17T03:10:05.246453Z",
     "shell.execute_reply": "2024-07-17T03:10:05.245763Z",
     "shell.execute_reply.started": "2024-07-17T03:10:05.164583Z"
    },
    "id": "shz4-RFOf41U"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dataset = load_dataset('json', data_files='df_sampled.json')\n",
    "# dataset = load_dataset(\"ealvaradob/phishing-dataset\", \"combined_reduced\", trust_remote_code=True)\n",
    "\n",
    "# Convert dataset to a pandas DataFrame\n",
    "# df = dataset['train'].to_pandas()\n",
    "\n",
    "# Split the DataFrame into train and test sets\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the train and test DataFrames back to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(set_1_df, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(set_3_df, preserve_index=False)\n",
    "\n",
    "# Combine the train and test Datasets into a DatasetDict\n",
    "dataset_split = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:10:08.109795Z",
     "iopub.status.busy": "2024-07-17T03:10:08.109474Z",
     "iopub.status.idle": "2024-07-17T03:10:08.114982Z",
     "shell.execute_reply": "2024-07-17T03:10:08.114224Z",
     "shell.execute_reply.started": "2024-07-17T03:10:08.109770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 20268\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5072\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWE97S_2I-tC"
   },
   "source": [
    "Decide on your pre-trained model along with your new model's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:11:03.799455Z",
     "iopub.status.busy": "2024-07-17T03:11:03.799118Z",
     "iopub.status.idle": "2024-07-17T03:11:03.803250Z",
     "shell.execute_reply": "2024-07-17T03:11:03.802489Z",
     "shell.execute_reply.started": "2024-07-17T03:11:03.799431Z"
    },
    "id": "GAIpslnnf6O6"
   },
   "outputs": [],
   "source": [
    "model_name = \"albert/albert-base-v2\"\n",
    "your_path = 'scenario_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DXTBviEKBia"
   },
   "source": [
    "Look over your distribution of the labels (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:11:15.116766Z",
     "iopub.status.busy": "2024-07-17T03:11:15.116442Z",
     "iopub.status.idle": "2024-07-17T03:11:15.135426Z",
     "shell.execute_reply": "2024-07-17T03:11:15.134825Z",
     "shell.execute_reply.started": "2024-07-17T03:11:15.116742Z"
    },
    "id": "nOvRUItff915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Distribution: Counter({0: 10290, 1: 9978})\n",
      "Test Label Distribution: Counter({1: 2585, 0: 2487})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_label_distribution = Counter(train_dataset['label'])\n",
    "test_label_distribution = Counter(test_dataset['label'])\n",
    "\n",
    "print(\"Training Label Distribution:\", train_label_distribution)\n",
    "print(\"Test Label Distribution:\", test_label_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZyWkHH0KIA_"
   },
   "source": [
    "Create a label encoder that converts categorical labels to a standardized numerical format. Labels in their original categorical form (e.g., 'clickbait', 'factual') need to be converted into numerical values so that they can be processed by the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:11:28.428213Z",
     "iopub.status.busy": "2024-07-17T03:11:28.427872Z",
     "iopub.status.idle": "2024-07-17T03:11:32.236046Z",
     "shell.execute_reply": "2024-07-17T03:11:32.235552Z",
     "shell.execute_reply.started": "2024-07-17T03:11:28.428188Z"
    },
    "id": "5PeJ0mwigBQ9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8a487feae84343886dae531a63da33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41906f3ef8e1493aa61ff68b0fd951ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(train_dataset['label'])\n",
    "\n",
    "def encode_labels(example):\n",
    "    return {'encoded_label': label_encoder.transform([example['label']])[0]}\n",
    "\n",
    "for split in dataset_split:\n",
    "    dataset_split[split] = dataset_split[split].map(encode_labels, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfKX7el9LEsq"
   },
   "source": [
    "The id2label and label2id mappings in AutoConfig are used to inform the model of the specific label-to-ID mappings so we can get the actual label names rather than the numerical reps when we do inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:11:51.486901Z",
     "iopub.status.busy": "2024-07-17T03:11:51.486423Z",
     "iopub.status.idle": "2024-07-17T03:11:56.693077Z",
     "shell.execute_reply": "2024-07-17T03:11:56.692462Z",
     "shell.execute_reply.started": "2024-07-17T03:11:51.486874Z"
    },
    "id": "_ECd3GJNgG-g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID to Label Mapping: {0: 'benign', 1: 'phishing'}\n",
      "Label to ID Mapping: {'benign': 0, 'phishing': 1}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# unique_labels = sorted(list(set(dataset_split['train']['label'])))\n",
    "# id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "# label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "id2label = {0: \"benign\", 1: \"phishing\"}\n",
    "label2id = {\"benign\": 0, \"phishing\": 1}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "# Verify the correct labels\n",
    "print(\"ID to Label Mapping:\", config.id2label)\n",
    "print(\"Label to ID Mapping:\", config.label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cTrvYVzLkoI"
   },
   "source": [
    "The provided code snippet is responsible for loading a tokenizer and a model from the Hugging Face Transformers library. Here we use ALBERT as a model, you can use AutoTokenizer and AutoModelForSequenceClassification if you want to use another model or it's specified tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:12:03.945248Z",
     "iopub.status.busy": "2024-07-17T03:12:03.944925Z",
     "iopub.status.idle": "2024-07-17T03:12:04.538365Z",
     "shell.execute_reply": "2024-07-17T03:12:04.537907Z",
     "shell.execute_reply.started": "2024-07-17T03:12:03.945224Z"
    },
    "id": "KJoTq1K8gwGV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "model = AlbertForSequenceClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOghCGAdMdqk"
   },
   "source": [
    "This next function makes sure the text data is properly tokenized and labeled, preparing the dataset for efficient training of the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:12:08.375546Z",
     "iopub.status.busy": "2024-07-17T03:12:08.375064Z",
     "iopub.status.idle": "2024-07-17T03:12:25.379408Z",
     "shell.execute_reply": "2024-07-17T03:12:25.378849Z",
     "shell.execute_reply.started": "2024-07-17T03:12:08.375521Z"
    },
    "id": "N4VJZjwhg38G"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58f77e28cb34c8f8e17e52a153fb721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6900cad374544194882d4cc47dfa538d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdaf95ca54f48ac8c8f12b87d05c8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be247635e8d47418d9f3520799a6375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_invalid_content(example):\n",
    "    return isinstance(example['text'], str)\n",
    "\n",
    "dataset = dataset_split.filter(filter_invalid_content, batched=False)\n",
    "\n",
    "# def encode_data(batch):\n",
    "#     tokenized_inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256)\n",
    "#     tokenized_inputs[\"labels\"] = batch[\"encoded_label\"]\n",
    "#     return tokenized_inputs\n",
    "\n",
    "# dataset_encoded = dataset.map(encode_data, batched=True)\n",
    "# # dataset_encoded\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "# Map the tokenization function to the dataset\n",
    "dataset_encoded = dataset_split.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:12:40.367043Z",
     "iopub.status.busy": "2024-07-17T03:12:40.366719Z",
     "iopub.status.idle": "2024-07-17T03:12:40.372148Z",
     "shell.execute_reply": "2024-07-17T03:12:40.371362Z",
     "shell.execute_reply.started": "2024-07-17T03:12:40.367018Z"
    },
    "id": "iQb-rRu9g5lT"
   },
   "outputs": [],
   "source": [
    "# Set the format for PyTorch\n",
    "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:12:43.814998Z",
     "iopub.status.busy": "2024-07-17T03:12:43.814657Z",
     "iopub.status.idle": "2024-07-17T03:12:43.819270Z",
     "shell.execute_reply": "2024-07-17T03:12:43.818500Z",
     "shell.execute_reply.started": "2024-07-17T03:12:43.814971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 20268\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 5072\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset to check the splits\n",
    "print(dataset_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-G5h1ET3NPLl"
   },
   "source": [
    "The DataCollatorWithPadding ensures that all input sequences in a batch are padded to the same length, using the padding logic defined by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:12:50.256859Z",
     "iopub.status.busy": "2024-07-17T03:12:50.256536Z",
     "iopub.status.idle": "2024-07-17T03:12:50.268668Z",
     "shell.execute_reply": "2024-07-17T03:12:50.268267Z",
     "shell.execute_reply.started": "2024-07-17T03:12:50.256835Z"
    },
    "id": "xg6Rb7R-g7A-"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSctqANNNaAc"
   },
   "source": [
    "Next we'll set up LabelEncoder to encode labels and defines a function to compute per-label accuracy from a confusion matrix, providing label-specific accuracy metrics. I.e. when we train the model we want to see the accuracy metrics per label as well as the average metrics. This is more relevant if you have more than two labels, and one is underperforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:12:53.870973Z",
     "iopub.status.busy": "2024-07-17T03:12:53.870655Z",
     "iopub.status.idle": "2024-07-17T03:12:53.877043Z",
     "shell.execute_reply": "2024-07-17T03:12:53.876388Z",
     "shell.execute_reply.started": "2024-07-17T03:12:53.870948Z"
    },
    "id": "zgMLYb57g-Lo"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "\n",
    "def per_label_accuracy(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    correct_predictions = cm.diagonal()\n",
    "    label_totals = cm.sum(axis=1)\n",
    "    per_label_acc = np.divide(correct_predictions, label_totals, out=np.zeros_like(correct_predictions, dtype=float), where=label_totals != 0)\n",
    "    return dict(zip(labels, per_label_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTeQeL8DN05z"
   },
   "source": [
    "Next we set up our compute metrics. Here I've set up several, but you may reduce them if needed be. You can read more on this metrics [here.](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:12:57.829751Z",
     "iopub.status.busy": "2024-07-17T03:12:57.829450Z",
     "iopub.status.idle": "2024-07-17T03:12:57.836933Z",
     "shell.execute_reply": "2024-07-17T03:12:57.836347Z",
     "shell.execute_reply.started": "2024-07-17T03:12:57.829725Z"
    },
    "id": "HInT5WReNvTS"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    decoded_labels = label_encoder.inverse_transform(labels)\n",
    "    decoded_preds = label_encoder.inverse_transform(preds)\n",
    "\n",
    "    precision = precision_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    recall = recall_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    acc = accuracy_score(decoded_labels, decoded_preds)\n",
    "\n",
    "    labels_list = list(label_encoder.classes_)\n",
    "    per_label_acc = per_label_accuracy(decoded_labels, decoded_preds, labels_list)\n",
    "\n",
    "    per_label_acc_metrics = {}\n",
    "    for label, accuracy in per_label_acc.items():\n",
    "        label_key = f\"accuracy_label_{label}\"\n",
    "        per_label_acc_metrics[label_key] = accuracy\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        **per_label_acc_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv3scyLpORAz"
   },
   "source": [
    "Lastly, we set up our training metrics to train the model. I'm following the paper [\"How to Fine-Tune BERT for Text Classification?\"](https://arxiv.org/abs/1905.05583) on epochs, batch size and learning rate but do play around with it if you want to.\n",
    "\n",
    "When it is in training, be sure to look out for training loss and validation loss. Both should decrease consistently. If validation is increasing consistently you may be overfitting your model and you can try to decrease number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:46:27.559793Z",
     "iopub.status.busy": "2024-07-17T03:46:27.559469Z",
     "iopub.status.idle": "2024-07-17T03:47:47.954552Z",
     "shell.execute_reply": "2024-07-17T03:47:47.953813Z",
     "shell.execute_reply.started": "2024-07-17T03:46:27.559769Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='951' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/951 01:18 < 03:54, 3.03 it/s, Epoch 0.75/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy Label 0</th>\n",
       "      <th>Accuracy Label 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330210</td>\n",
       "      <td>0.964511</td>\n",
       "      <td>0.964504</td>\n",
       "      <td>0.965885</td>\n",
       "      <td>0.964511</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.938878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331230</td>\n",
       "      <td>0.965694</td>\n",
       "      <td>0.965690</td>\n",
       "      <td>0.966841</td>\n",
       "      <td>0.965694</td>\n",
       "      <td>0.989948</td>\n",
       "      <td>0.942360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 28\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39myour_path,\n\u001b[1;32m      5\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2273\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=your_path,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=1000,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded['train'],\n",
    "    eval_dataset=dataset_encoded['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:37:06.492473Z",
     "iopub.status.busy": "2024-07-17T03:37:06.492149Z",
     "iopub.status.idle": "2024-07-17T03:44:19.516018Z",
     "shell.execute_reply": "2024-07-17T03:44:19.515416Z",
     "shell.execute_reply.started": "2024-07-17T03:37:06.492449Z"
    },
    "id": "yx_etZ24hBjt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1268' max='1268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1268/1268 07:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy Label 0</th>\n",
       "      <th>Accuracy Label 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.493434</td>\n",
       "      <td>0.941838</td>\n",
       "      <td>0.941725</td>\n",
       "      <td>0.947276</td>\n",
       "      <td>0.941838</td>\n",
       "      <td>0.996381</td>\n",
       "      <td>0.889362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311369</td>\n",
       "      <td>0.959582</td>\n",
       "      <td>0.959563</td>\n",
       "      <td>0.961680</td>\n",
       "      <td>0.959582</td>\n",
       "      <td>0.992762</td>\n",
       "      <td>0.927660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.159747</td>\n",
       "      <td>0.970623</td>\n",
       "      <td>0.970621</td>\n",
       "      <td>0.970640</td>\n",
       "      <td>0.970623</td>\n",
       "      <td>0.966626</td>\n",
       "      <td>0.974468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.251982</td>\n",
       "      <td>0.968651</td>\n",
       "      <td>0.968651</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.968651</td>\n",
       "      <td>0.988339</td>\n",
       "      <td>0.949710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.171561</td>\n",
       "      <td>0.967666</td>\n",
       "      <td>0.967668</td>\n",
       "      <td>0.968127</td>\n",
       "      <td>0.967666</td>\n",
       "      <td>0.982710</td>\n",
       "      <td>0.953191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.180714</td>\n",
       "      <td>0.970820</td>\n",
       "      <td>0.970823</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>0.970820</td>\n",
       "      <td>0.981102</td>\n",
       "      <td>0.960928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.248228</td>\n",
       "      <td>0.966680</td>\n",
       "      <td>0.966681</td>\n",
       "      <td>0.967201</td>\n",
       "      <td>0.966680</td>\n",
       "      <td>0.982710</td>\n",
       "      <td>0.951257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.203609</td>\n",
       "      <td>0.967666</td>\n",
       "      <td>0.967667</td>\n",
       "      <td>0.967675</td>\n",
       "      <td>0.967666</td>\n",
       "      <td>0.969039</td>\n",
       "      <td>0.966344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.295926</td>\n",
       "      <td>0.954850</td>\n",
       "      <td>0.954821</td>\n",
       "      <td>0.957395</td>\n",
       "      <td>0.954850</td>\n",
       "      <td>0.991556</td>\n",
       "      <td>0.919536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337719</td>\n",
       "      <td>0.958991</td>\n",
       "      <td>0.958969</td>\n",
       "      <td>0.961211</td>\n",
       "      <td>0.958991</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.926112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284508</td>\n",
       "      <td>0.964905</td>\n",
       "      <td>0.964900</td>\n",
       "      <td>0.966125</td>\n",
       "      <td>0.964905</td>\n",
       "      <td>0.989948</td>\n",
       "      <td>0.940812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295480</td>\n",
       "      <td>0.964314</td>\n",
       "      <td>0.964307</td>\n",
       "      <td>0.965668</td>\n",
       "      <td>0.964314</td>\n",
       "      <td>0.990752</td>\n",
       "      <td>0.938878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1268, training_loss=0.006302215813959167, metrics={'train_runtime': 432.4407, 'train_samples_per_second': 187.475, 'train_steps_per_second': 2.932, 'total_flos': 968733452943360.0, 'train_loss': 0.006302215813959167, 'epoch': 4.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=your_path,\n",
    "    num_train_epochs=4,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=1000,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded['train'],\n",
    "    eval_dataset=dataset_encoded['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7L3GQsHPDRL"
   },
   "source": [
    "Once you're finito, you can evaluate the results, save your model and the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:25:35.147320Z",
     "iopub.status.busy": "2024-07-17T03:25:35.146976Z",
     "iopub.status.idle": "2024-07-17T03:25:43.661567Z",
     "shell.execute_reply": "2024-07-17T03:25:43.661073Z",
     "shell.execute_reply.started": "2024-07-17T03:25:35.147293Z"
    },
    "id": "lCiYYKyqhFL0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [159/159 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "trainer.save_model(your_path)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vim7CVwhPKY4"
   },
   "source": [
    "If you want to test it out, you can run the pipeline directly with the model. I just used some new example titles to see how it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:27:20.098088Z",
     "iopub.status.busy": "2024-07-17T03:27:20.097409Z",
     "iopub.status.idle": "2024-07-17T03:27:20.506923Z",
     "shell.execute_reply": "2024-07-17T03:27:20.506546Z",
     "shell.execute_reply.started": "2024-07-17T03:27:20.098061Z"
    },
    "id": "pTvsu7rOhYTL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('text-classification', model='scenario_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:27:23.446843Z",
     "iopub.status.busy": "2024-07-17T03:27:23.445934Z",
     "iopub.status.idle": "2024-07-17T03:27:23.618087Z",
     "shell.execute_reply": "2024-07-17T03:27:23.617550Z",
     "shell.execute_reply.started": "2024-07-17T03:27:23.446816Z"
    },
    "id": "wmG1MgK1haml",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sex up ur mobile with a FREE sexy pic of Aho! Just text BABE to 88600. Then every wk get a sexy celeb! PocketBabe.co.uk 4 more pics. 16 £3/wk 087016248\n",
      "Output: phishing\n",
      "Title: Pity.  Reading that woman's ad and knowing Rohit for years, they sound like a match made in heaven.  But why, oh, why, keep that shaved-head photo on prominent display???  There are lots of photos of Rohit looking rather dashing, and with the crucial hair feature enabled!R\n",
      "Output: benign\n"
     ]
    }
   ],
   "source": [
    "example_titles = [\n",
    "    \"Sex up ur mobile with a FREE sexy pic of Aho! Just text BABE to 88600. Then every wk get a sexy celeb! PocketBabe.co.uk 4 more pics. 16 \\u00a33/wk 087016248\",\n",
    "    \"Pity.  Reading that woman's ad and knowing Rohit for years, they sound like a match made in heaven.  But why, oh, why, keep that shaved-head photo on prominent display???  There are lots of photos of Rohit looking rather dashing, and with the crucial hair feature enabled!R\",\n",
    "]\n",
    "\n",
    "for title in example_titles:\n",
    "    result = pipe(title)\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Output: {result[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bscp7lvwPW7N"
   },
   "source": [
    "If you're satisfied, you can log in to HuggingFace with a token (you'll find these in your account under Settings - make sure it has write access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:27:28.538475Z",
     "iopub.status.busy": "2024-07-17T03:27:28.537803Z",
     "iopub.status.idle": "2024-07-17T03:27:34.354436Z",
     "shell.execute_reply": "2024-07-17T03:27:34.353455Z",
     "shell.execute_reply.started": "2024-07-17T03:27:28.538448Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P98pcG_LPhoV"
   },
   "source": [
    "Push the model with your new name for it. It usually just takes the name you set when you trained it so whatever you put here doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T03:27:35.177359Z",
     "iopub.status.busy": "2024-07-17T03:27:35.176865Z",
     "iopub.status.idle": "2024-07-17T03:27:46.712577Z",
     "shell.execute_reply": "2024-07-17T03:27:46.711814Z",
     "shell.execute_reply.started": "2024-07-17T03:27:35.177329Z"
    },
    "id": "pVE_898bhhTR",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8890864b8a934b4d9081966a6a25c9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd9a0ba9bc34930b5bcef1f0f711b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88986a78c5b94f0a89a1f976a7e2eca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f93a092bab45d292c201c0583c7320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jordan2889/scenario_2/commit/deaf4a57918fe6acff8d1d360c17b28d2193e766', commit_message='jordan2889/scenario_2', commit_description='', oid='deaf4a57918fe6acff8d1d360c17b28d2193e766', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"jordan2889/scenario_2\")\n",
    "trainer.push_to_hub(\"jordan2889/scenario_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUokttBHPvoc"
   },
   "source": [
    "Now, you're done. You got your text classifier."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN3C3XgYBmqGPgFOx823pPs",
   "gpuType": "L4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
